{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Covid-19 Tweets NLP using Tensorflow","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-06T18:14:25.55977Z","iopub.execute_input":"2021-11-06T18:14:25.560271Z","iopub.status.idle":"2021-11-06T18:14:25.596039Z","shell.execute_reply.started":"2021-11-06T18:14:25.560128Z","shell.execute_reply":"2021-11-06T18:14:25.59487Z"}}},{"cell_type":"markdown","source":"For this project we are trying classify the sentiment of tweets regarding Covid-19. \\\nWe are going to use Tensorflow and Keras to perform NLP, more specifically we will be using techinques such as:\n\n* Tokenizing\n* Padding,\n* Embedding\n* GRU\n* LSTM\n* Convolutions\n* Dropout \n\nThere will also be some data cleaning and reduction of vocabulary to increase the performance of our models.\n\n We will achieve about 88% accuracy on the validation data.","metadata":{}},{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:10.315971Z","iopub.execute_input":"2021-11-08T13:41:10.316323Z","iopub.status.idle":"2021-11-08T13:41:10.322346Z","shell.execute_reply.started":"2021-11-08T13:41:10.316286Z","shell.execute_reply":"2021-11-08T13:41:10.321631Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Import the data","metadata":{}},{"cell_type":"code","source":"df_training = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_train.csv', encoding = 'latin_1')\ndf_validation = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_test.csv', encoding = 'latin_1')","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:10.324253Z","iopub.execute_input":"2021-11-08T13:41:10.324535Z","iopub.status.idle":"2021-11-08T13:41:10.544816Z","shell.execute_reply.started":"2021-11-08T13:41:10.324504Z","shell.execute_reply":"2021-11-08T13:41:10.543833Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df_training.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:10.546324Z","iopub.execute_input":"2021-11-08T13:41:10.546699Z","iopub.status.idle":"2021-11-08T13:41:10.561785Z","shell.execute_reply.started":"2021-11-08T13:41:10.546664Z","shell.execute_reply":"2021-11-08T13:41:10.560570Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_validation.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:10.564285Z","iopub.execute_input":"2021-11-08T13:41:10.564539Z","iopub.status.idle":"2021-11-08T13:41:10.578274Z","shell.execute_reply.started":"2021-11-08T13:41:10.564512Z","shell.execute_reply":"2021-11-08T13:41:10.577612Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"training = df_training.copy()\nvalidation = df_validation.copy()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:10.580125Z","iopub.execute_input":"2021-11-08T13:41:10.581050Z","iopub.status.idle":"2021-11-08T13:41:10.593077Z","shell.execute_reply.started":"2021-11-08T13:41:10.581011Z","shell.execute_reply":"2021-11-08T13:41:10.592091Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"We will only be using the Original Tweet column to predict the Sentiment.","metadata":{}},{"cell_type":"code","source":"training = training[['OriginalTweet', 'Sentiment']]\nvalidation = validation[['OriginalTweet', 'Sentiment']]","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:10.594241Z","iopub.execute_input":"2021-11-08T13:41:10.594478Z","iopub.status.idle":"2021-11-08T13:41:10.607287Z","shell.execute_reply.started":"2021-11-08T13:41:10.594451Z","shell.execute_reply":"2021-11-08T13:41:10.606435Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Missing Values\nLets check for any missing values.","metadata":{}},{"cell_type":"code","source":"training.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:10.608484Z","iopub.execute_input":"2021-11-08T13:41:10.608751Z","iopub.status.idle":"2021-11-08T13:41:10.636387Z","shell.execute_reply.started":"2021-11-08T13:41:10.608723Z","shell.execute_reply":"2021-11-08T13:41:10.635355Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"validation.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:10.637676Z","iopub.execute_input":"2021-11-08T13:41:10.637888Z","iopub.status.idle":"2021-11-08T13:41:10.651944Z","shell.execute_reply.started":"2021-11-08T13:41:10.637862Z","shell.execute_reply":"2021-11-08T13:41:10.651034Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Take a look on the different sentiment categories.","metadata":{}},{"cell_type":"code","source":"sns.catplot(x = 'Sentiment', kind = 'count', data = training, height = 5, aspect = 2)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:10.653276Z","iopub.execute_input":"2021-11-08T13:41:10.653926Z","iopub.status.idle":"2021-11-08T13:41:11.055313Z","shell.execute_reply.started":"2021-11-08T13:41:10.653887Z","shell.execute_reply":"2021-11-08T13:41:11.054376Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"sns.catplot(x = 'Sentiment', kind = 'count', data = validation, height = 5, aspect = 2)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:11.056742Z","iopub.execute_input":"2021-11-08T13:41:11.057027Z","iopub.status.idle":"2021-11-08T13:41:11.374842Z","shell.execute_reply.started":"2021-11-08T13:41:11.056989Z","shell.execute_reply":"2021-11-08T13:41:11.374069Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Mapping the Sentiment Column\nNow we recode the Sentiment column into numerical values. We will only use 3 categories, Negative, Neutral and Positive.","metadata":{}},{"cell_type":"code","source":"l = {'Extremely Negative': 0,\n     'Negative': 0,\n     'Neutral': 1,\n     'Positive': 2,\n     'Extremely Positive': 2}\n","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:11.376278Z","iopub.execute_input":"2021-11-08T13:41:11.376760Z","iopub.status.idle":"2021-11-08T13:41:11.381337Z","shell.execute_reply.started":"2021-11-08T13:41:11.376724Z","shell.execute_reply":"2021-11-08T13:41:11.380333Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"training['Sentiment'] = training['Sentiment'].map(l)\nvalidation['Sentiment'] = validation['Sentiment'].map(l)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:11.383176Z","iopub.execute_input":"2021-11-08T13:41:11.383445Z","iopub.status.idle":"2021-11-08T13:41:11.404866Z","shell.execute_reply.started":"2021-11-08T13:41:11.383414Z","shell.execute_reply":"2021-11-08T13:41:11.403905Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"training['Sentiment'].value_counts","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:11.408394Z","iopub.execute_input":"2021-11-08T13:41:11.408778Z","iopub.status.idle":"2021-11-08T13:41:11.417984Z","shell.execute_reply.started":"2021-11-08T13:41:11.408739Z","shell.execute_reply":"2021-11-08T13:41:11.417147Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"validation['Sentiment'].value_counts","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:11.419450Z","iopub.execute_input":"2021-11-08T13:41:11.420038Z","iopub.status.idle":"2021-11-08T13:41:11.432882Z","shell.execute_reply.started":"2021-11-08T13:41:11.419997Z","shell.execute_reply":"2021-11-08T13:41:11.432077Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning\nNow we clean the tweets by removing the urls and the people tagged from them. Also we are going to remove tweets that are shorter than 20 characters from the training set.","metadata":{}},{"cell_type":"code","source":"training['OriginalTweet'] = training['OriginalTweet'].str.replace(r'http\\S+', '', regex = True)\ntraining['OriginalTweet'] = training['OriginalTweet'].str.replace(r'@\\S+', '', regex = True)\nvalidation['OriginalTweet'] = validation['OriginalTweet'].str.replace(r'http\\S+', '', regex = True)\nvalidation['OriginalTweet'] = validation['OriginalTweet'].str.replace(r'@\\S+', '', regex = True)\nprint(training['OriginalTweet'])","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:11.434181Z","iopub.execute_input":"2021-11-08T13:41:11.434438Z","iopub.status.idle":"2021-11-08T13:41:11.585991Z","shell.execute_reply.started":"2021-11-08T13:41:11.434408Z","shell.execute_reply":"2021-11-08T13:41:11.584806Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print(training.shape)\ntraining = training[(training['OriginalTweet'].str.len() > 20)]\nprint(training.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:11.587355Z","iopub.execute_input":"2021-11-08T13:41:11.587701Z","iopub.status.idle":"2021-11-08T13:41:11.635864Z","shell.execute_reply.started":"2021-11-08T13:41:11.587668Z","shell.execute_reply":"2021-11-08T13:41:11.634752Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize\nFor the tokenization everything is quite standard, we tokenize and then we pad all the sentences so that they have the same length. The max length of a sentence is 120 words which sufficient. We have decreased the vocabulary of the Tokenizer from about 50000 to 6000 in order to reduce overfitting since this will make vocabulary consist of only the 6000 most common words from the training data. ","metadata":{}},{"cell_type":"code","source":"embedding_dim = 16\nmax_length = 120\ntrunc_type = 'post'\noov_tok = '<OOV>'\n\ntraining_sentences = training['OriginalTweet']\ntraining_labels = training['Sentiment']\n\nvalidation_sentences = validation['OriginalTweet']\nvalidation_labels = validation['Sentiment']\n\ntokenizer = Tokenizer(oov_token = oov_tok, num_words = 6000)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\nvocab_size = len(word_index) +1\n\nsequences = tokenizer.texts_to_sequences(training_sentences)\npadded = pad_sequences(sequences, maxlen = max_length, truncating = trunc_type)\n\nvalidation_sequences = tokenizer.texts_to_sequences(validation_sentences)\nvalidation_padded = pad_sequences(validation_sequences, maxlen = max_length, truncating = trunc_type)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:11.637037Z","iopub.execute_input":"2021-11-08T13:41:11.637284Z","iopub.status.idle":"2021-11-08T13:41:15.391229Z","shell.execute_reply.started":"2021-11-08T13:41:11.637253Z","shell.execute_reply":"2021-11-08T13:41:15.390619Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Models\nI am going to try to different models, one using GRU and one using LSTM.","metadata":{}},{"cell_type":"markdown","source":"I am going to use a callback so that the training stops after 5 epochs if the validation accuarcy is not increasing. That should remove the problem of overfitting. Also the model will restore the weights for the epoch with the best validation accuracy.","metadata":{}},{"cell_type":"code","source":"callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', patience = 5, restore_best_weights = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:15.392199Z","iopub.execute_input":"2021-11-08T13:41:15.393108Z","iopub.status.idle":"2021-11-08T13:41:15.398034Z","shell.execute_reply.started":"2021-11-08T13:41:15.393066Z","shell.execute_reply":"2021-11-08T13:41:15.396637Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## GRU","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n    tf.keras.layers.Dropout(0.5),\n    \n    tf.keras.layers.Conv1D(128, 5, activation = 'relu'),\n    tf.keras.layers.MaxPooling1D(pool_size = 1),\n    tf.keras.layers.Dropout(0.3),\n    \n    tf.keras.layers.Conv1D(256, 5, activation = 'relu'),\n    tf.keras.layers.Dropout(0.3),\n    \n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(64, activation = 'relu'),\n    tf.keras.layers.Dense(3, activation = 'softmax')\n])\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:15.400462Z","iopub.execute_input":"2021-11-08T13:41:15.401205Z","iopub.status.idle":"2021-11-08T13:41:16.112441Z","shell.execute_reply.started":"2021-11-08T13:41:15.401152Z","shell.execute_reply":"2021-11-08T13:41:16.111636Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"history = model.fit(padded, \n                    np.array(training_labels), \n                    epochs = 20, \n                    validation_data = (validation_padded, np.array(validation_labels)),\n                    callbacks = [callback])","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:41:16.114046Z","iopub.execute_input":"2021-11-08T13:41:16.114521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = history.history['loss']\nacc = history.history['accuracy']\n\nval_loss = history.history['val_loss']\nval_acc = history.history['val_accuracy']\n\nplt.plot(loss, label = 'Training Loss')\nplt.plot(val_loss, label = 'Validation Loss')\nplt.title('Loss Plot')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.figure()\n\nplt.plot(acc, label = 'Training Accuracy')\nplt.plot(val_acc, label = 'Validation Accuracy')\nplt.title('Accuracy Plot')\nplt.xlabel('Epochs')\nplt.legend()\nplt.ylabel('Accuarcy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While our accuracy on the training data is increasing our accuracy on the validation data is decreasing/stagnant which is sign that we are overfitting.\nThe callback went in and stoppped the training early.","metadata":{}},{"cell_type":"markdown","source":"## LSTM","metadata":{}},{"cell_type":"code","source":"model_2 = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, 64, input_length = max_length),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv1D(128, 5, activation = 'relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv1D(256, 5, activation = 'relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv1D(512, 5, activation = 'relu'),\n    tf.keras.layers.MaxPooling1D(pool_size = 4),\n    tf.keras.layers.LSTM(128),\n    tf.keras.layers.Dense(64, activation = 'relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(32, activation = 'relu'),\n    tf.keras.layers.Dense(3, activation = 'softmax')\n])\nmodel_2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adam = tf.keras.optimizers.Adam(\n    learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n    name='Adam'\n)\nmodel_2.compile(loss = 'sparse_categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\nhistory_2 = model_2.fit(padded,\n                        training_labels,\n                        epochs = 20,\n                        validation_data = (validation_padded, validation_labels), \n                        callbacks = [callback]\n                       )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nloss = history_2.history['loss']\nacc = history_2.history['accuracy']\n\nval_loss = history_2.history['val_loss']\nval_acc = history_2.history['val_accuracy']\n\n\n\nplt.plot(loss, label = 'Training Loss')\nplt.plot(val_loss, label = 'Validation loss')\nplt.title('Loss Plot')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.figure()\n\nplt.plot(acc, label = 'Training Accuracy')\nplt.plot(val_acc, label = 'Validation Accuracy')\nplt.title('Accuracy Plot')\nplt.ylabel('Accuarcy')\nplt.legend()\nplt.xlabel('Epochs')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}