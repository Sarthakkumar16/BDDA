{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-06T18:14:25.560271Z",
     "iopub.status.busy": "2021-11-06T18:14:25.55977Z",
     "iopub.status.idle": "2021-11-06T18:14:25.596039Z",
     "shell.execute_reply": "2021-11-06T18:14:25.59487Z",
     "shell.execute_reply.started": "2021-11-06T18:14:25.560128Z"
    }
   },
   "source": [
    "# Covid-19 Tweets NLP using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we are trying classify the sentiment of tweets regarding Covid-19. \\\n",
    "We are going to use Tensorflow and Keras to perform NLP, more specifically we will be using techinques such as:\n",
    "\n",
    "* Tokenizing\n",
    "* Padding,\n",
    "* Embedding\n",
    "* GRU\n",
    "* LSTM\n",
    "* Convolutions\n",
    "* Dropout \n",
    "\n",
    "There will also be some data cleaning and reduction of vocabulary to increase the performance of our models.\n",
    "\n",
    " We will achieve about 88% accuracy on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:10.316323Z",
     "iopub.status.busy": "2021-11-08T13:41:10.315971Z",
     "iopub.status.idle": "2021-11-08T13:41:10.322346Z",
     "shell.execute_reply": "2021-11-08T13:41:10.321631Z",
     "shell.execute_reply.started": "2021-11-08T13:41:10.316286Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a25f0b540550>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:10.324535Z",
     "iopub.status.busy": "2021-11-08T13:41:10.324253Z",
     "iopub.status.idle": "2021-11-08T13:41:10.544816Z",
     "shell.execute_reply": "2021-11-08T13:41:10.543833Z",
     "shell.execute_reply.started": "2021-11-08T13:41:10.324504Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/covid-19-nlp-text-classification/Corona_NLP_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-94cded9194c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/covid-19-nlp-text-classification/Corona_NLP_train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'latin_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/covid-19-nlp-text-classification/Corona_NLP_test.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'latin_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1989\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1990\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1991\u001b[1;33m                 \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1992\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/covid-19-nlp-text-classification/Corona_NLP_train.csv'"
     ]
    }
   ],
   "source": [
    "df_training = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_train.csv', encoding = 'latin_1')\n",
    "df_validation = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_test.csv', encoding = 'latin_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:10.546699Z",
     "iopub.status.busy": "2021-11-08T13:41:10.546324Z",
     "iopub.status.idle": "2021-11-08T13:41:10.561785Z",
     "shell.execute_reply": "2021-11-08T13:41:10.560570Z",
     "shell.execute_reply.started": "2021-11-08T13:41:10.546664Z"
    }
   },
   "outputs": [],
   "source": [
    "df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:10.564539Z",
     "iopub.status.busy": "2021-11-08T13:41:10.564285Z",
     "iopub.status.idle": "2021-11-08T13:41:10.578274Z",
     "shell.execute_reply": "2021-11-08T13:41:10.577612Z",
     "shell.execute_reply.started": "2021-11-08T13:41:10.564512Z"
    }
   },
   "outputs": [],
   "source": [
    "df_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:10.581050Z",
     "iopub.status.busy": "2021-11-08T13:41:10.580125Z",
     "iopub.status.idle": "2021-11-08T13:41:10.593077Z",
     "shell.execute_reply": "2021-11-08T13:41:10.592091Z",
     "shell.execute_reply.started": "2021-11-08T13:41:10.581011Z"
    }
   },
   "outputs": [],
   "source": [
    "training = df_training.copy()\n",
    "validation = df_validation.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only be using the Original Tweet column to predict the Sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:10.594478Z",
     "iopub.status.busy": "2021-11-08T13:41:10.594241Z",
     "iopub.status.idle": "2021-11-08T13:41:10.607287Z",
     "shell.execute_reply": "2021-11-08T13:41:10.606435Z",
     "shell.execute_reply.started": "2021-11-08T13:41:10.594451Z"
    }
   },
   "outputs": [],
   "source": [
    "training = training[['OriginalTweet', 'Sentiment']]\n",
    "validation = validation[['OriginalTweet', 'Sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "Lets check for any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:10.608751Z",
     "iopub.status.busy": "2021-11-08T13:41:10.608484Z",
     "iopub.status.idle": "2021-11-08T13:41:10.636387Z",
     "shell.execute_reply": "2021-11-08T13:41:10.635355Z",
     "shell.execute_reply.started": "2021-11-08T13:41:10.608723Z"
    }
   },
   "outputs": [],
   "source": [
    "training.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:10.637888Z",
     "iopub.status.busy": "2021-11-08T13:41:10.637676Z",
     "iopub.status.idle": "2021-11-08T13:41:10.651944Z",
     "shell.execute_reply": "2021-11-08T13:41:10.651034Z",
     "shell.execute_reply.started": "2021-11-08T13:41:10.637862Z"
    }
   },
   "outputs": [],
   "source": [
    "validation.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look on the different sentiment categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:10.653926Z",
     "iopub.status.busy": "2021-11-08T13:41:10.653276Z",
     "iopub.status.idle": "2021-11-08T13:41:11.055313Z",
     "shell.execute_reply": "2021-11-08T13:41:11.054376Z",
     "shell.execute_reply.started": "2021-11-08T13:41:10.653887Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.catplot(x = 'Sentiment', kind = 'count', data = training, height = 5, aspect = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:11.057027Z",
     "iopub.status.busy": "2021-11-08T13:41:11.056742Z",
     "iopub.status.idle": "2021-11-08T13:41:11.374842Z",
     "shell.execute_reply": "2021-11-08T13:41:11.374069Z",
     "shell.execute_reply.started": "2021-11-08T13:41:11.056989Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.catplot(x = 'Sentiment', kind = 'count', data = validation, height = 5, aspect = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping the Sentiment Column\n",
    "Now we recode the Sentiment column into numerical values. We will only use 3 categories, Negative, Neutral and Positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:11.376760Z",
     "iopub.status.busy": "2021-11-08T13:41:11.376278Z",
     "iopub.status.idle": "2021-11-08T13:41:11.381337Z",
     "shell.execute_reply": "2021-11-08T13:41:11.380333Z",
     "shell.execute_reply.started": "2021-11-08T13:41:11.376724Z"
    }
   },
   "outputs": [],
   "source": [
    "l = {'Extremely Negative': 0,\n",
    "     'Negative': 0,\n",
    "     'Neutral': 1,\n",
    "     'Positive': 2,\n",
    "     'Extremely Positive': 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:11.383445Z",
     "iopub.status.busy": "2021-11-08T13:41:11.383176Z",
     "iopub.status.idle": "2021-11-08T13:41:11.404866Z",
     "shell.execute_reply": "2021-11-08T13:41:11.403905Z",
     "shell.execute_reply.started": "2021-11-08T13:41:11.383414Z"
    }
   },
   "outputs": [],
   "source": [
    "training['Sentiment'] = training['Sentiment'].map(l)\n",
    "validation['Sentiment'] = validation['Sentiment'].map(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:11.408778Z",
     "iopub.status.busy": "2021-11-08T13:41:11.408394Z",
     "iopub.status.idle": "2021-11-08T13:41:11.417984Z",
     "shell.execute_reply": "2021-11-08T13:41:11.417147Z",
     "shell.execute_reply.started": "2021-11-08T13:41:11.408739Z"
    }
   },
   "outputs": [],
   "source": [
    "training['Sentiment'].value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:11.420038Z",
     "iopub.status.busy": "2021-11-08T13:41:11.419450Z",
     "iopub.status.idle": "2021-11-08T13:41:11.432882Z",
     "shell.execute_reply": "2021-11-08T13:41:11.432077Z",
     "shell.execute_reply.started": "2021-11-08T13:41:11.419997Z"
    }
   },
   "outputs": [],
   "source": [
    "validation['Sentiment'].value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "Now we clean the tweets by removing the urls and the people tagged from them. Also we are going to remove tweets that are shorter than 20 characters from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:11.434438Z",
     "iopub.status.busy": "2021-11-08T13:41:11.434181Z",
     "iopub.status.idle": "2021-11-08T13:41:11.585991Z",
     "shell.execute_reply": "2021-11-08T13:41:11.584806Z",
     "shell.execute_reply.started": "2021-11-08T13:41:11.434408Z"
    }
   },
   "outputs": [],
   "source": [
    "training['OriginalTweet'] = training['OriginalTweet'].str.replace(r'http\\S+', '', regex = True)\n",
    "training['OriginalTweet'] = training['OriginalTweet'].str.replace(r'@\\S+', '', regex = True)\n",
    "validation['OriginalTweet'] = validation['OriginalTweet'].str.replace(r'http\\S+', '', regex = True)\n",
    "validation['OriginalTweet'] = validation['OriginalTweet'].str.replace(r'@\\S+', '', regex = True)\n",
    "print(training['OriginalTweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:11.587701Z",
     "iopub.status.busy": "2021-11-08T13:41:11.587355Z",
     "iopub.status.idle": "2021-11-08T13:41:11.635864Z",
     "shell.execute_reply": "2021-11-08T13:41:11.634752Z",
     "shell.execute_reply.started": "2021-11-08T13:41:11.587668Z"
    }
   },
   "outputs": [],
   "source": [
    "print(training.shape)\n",
    "training = training[(training['OriginalTweet'].str.len() > 20)]\n",
    "print(training.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "For the tokenization everything is quite standard, we tokenize and then we pad all the sentences so that they have the same length. The max length of a sentence is 120 words which sufficient. We have decreased the vocabulary of the Tokenizer from about 50000 to 6000 in order to reduce overfitting since this will make vocabulary consist of only the 6000 most common words from the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:11.637284Z",
     "iopub.status.busy": "2021-11-08T13:41:11.637037Z",
     "iopub.status.idle": "2021-11-08T13:41:15.391229Z",
     "shell.execute_reply": "2021-11-08T13:41:15.390619Z",
     "shell.execute_reply.started": "2021-11-08T13:41:11.637253Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "training_sentences = training['OriginalTweet']\n",
    "training_labels = training['Sentiment']\n",
    "\n",
    "validation_sentences = validation['OriginalTweet']\n",
    "validation_labels = validation['Sentiment']\n",
    "\n",
    "tokenizer = Tokenizer(oov_token = oov_tok, num_words = 6000)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) +1\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences, maxlen = max_length, truncating = trunc_type)\n",
    "\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen = max_length, truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "I am going to try to different models, one using GRU and one using LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to use a callback so that the training stops after 5 epochs if the validation accuarcy is not increasing. That should remove the problem of overfitting. Also the model will restore the weights for the epoch with the best validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:15.393108Z",
     "iopub.status.busy": "2021-11-08T13:41:15.392199Z",
     "iopub.status.idle": "2021-11-08T13:41:15.398034Z",
     "shell.execute_reply": "2021-11-08T13:41:15.396637Z",
     "shell.execute_reply.started": "2021-11-08T13:41:15.393066Z"
    }
   },
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', patience = 5, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:15.401205Z",
     "iopub.status.busy": "2021-11-08T13:41:15.400462Z",
     "iopub.status.idle": "2021-11-08T13:41:16.112441Z",
     "shell.execute_reply": "2021-11-08T13:41:16.111636Z",
     "shell.execute_reply.started": "2021-11-08T13:41:15.401152Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Conv1D(128, 5, activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 1),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    tf.keras.layers.Conv1D(256, 5, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(3, activation = 'softmax')\n",
    "])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:41:16.114521Z",
     "iopub.status.busy": "2021-11-08T13:41:16.114046Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(padded, \n",
    "                    np.array(training_labels), \n",
    "                    epochs = 20, \n",
    "                    validation_data = (validation_padded, np.array(validation_labels)),\n",
    "                    callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "acc = history.history['accuracy']\n",
    "\n",
    "val_loss = history.history['val_loss']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(loss, label = 'Training Loss')\n",
    "plt.plot(val_loss, label = 'Validation Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(acc, label = 'Training Accuracy')\n",
    "plt.plot(val_acc, label = 'Validation Accuracy')\n",
    "plt.title('Accuracy Plot')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.ylabel('Accuarcy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our accuracy on the training data is increasing our accuracy on the validation data is decreasing/stagnant which is sign that we are overfitting.\n",
    "The callback went in and stoppped the training early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 64, input_length = max_length),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv1D(128, 5, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv1D(256, 5, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv1D(512, 5, activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 4),\n",
    "    tf.keras.layers.LSTM(128),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(3, activation = 'softmax')\n",
    "])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam'\n",
    ")\n",
    "model_2.compile(loss = 'sparse_categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "history_2 = model_2.fit(padded,\n",
    "                        training_labels,\n",
    "                        epochs = 20,\n",
    "                        validation_data = (validation_padded, validation_labels), \n",
    "                        callbacks = [callback]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = history_2.history['loss']\n",
    "acc = history_2.history['accuracy']\n",
    "\n",
    "val_loss = history_2.history['val_loss']\n",
    "val_acc = history_2.history['val_accuracy']\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(loss, label = 'Training Loss')\n",
    "plt.plot(val_loss, label = 'Validation loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(acc, label = 'Training Accuracy')\n",
    "plt.plot(val_acc, label = 'Validation Accuracy')\n",
    "plt.title('Accuracy Plot')\n",
    "plt.ylabel('Accuarcy')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
